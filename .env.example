# =============================================================================
# OpenAI Configuration (Legacy Mode)
# =============================================================================
# Used when running in legacy mode (without --multi-model flag)
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=gpt-5.1
OPENAI_TEMPERATURE=0.0

# =============================================================================
# Multi-Model Configuration
# =============================================================================
# Provider API Keys (set the ones you need)
# OPENAI_KEY=sk-...                         # Alternative to OPENAI_API_KEY
# ANTHROPIC_KEY=sk-ant-...                  # For Anthropic Claude models
# KIMI_KEY=...                              # For Kimi/Moonshot AI
# LOCAL_LLM_KEY=...                         # For local models (vLLM, etc.)
# GOOGLE_API_KEY=...                        # For Google Gemini

# Global default model (used if component-specific not set)
DEFAULT_MODEL=gpt-5.1

# Agent-specific model assignments
# MAIN_AGENT_MODEL=gpt-5.1                   # Main orchestrator agent
# BROWSER_AGENT_MODEL=gpt-5.1-mini           # Browser navigation agent
# SELECTOR_AGENT_MODEL=gpt-5.1               # CSS selector discovery agent
# ACCESSIBILITY_AGENT_MODEL=gpt-5.1-mini     # HTTP accessibility check agent
# DATA_PREP_AGENT_MODEL=gpt-5.1-mini         # Test data preparation agent

# Tool-specific model assignments
# LISTING_PAGE_EXTRACTOR_MODEL=gpt-5.1       # Extract selectors from listing pages
# ARTICLE_PAGE_EXTRACTOR_MODEL=gpt-5.1       # Extract selectors from article pages
# SELECTOR_AGGREGATOR_MODEL=gpt-5.1          # Aggregate selector patterns
# LISTING_PAGES_GENERATOR_MODEL=gpt-5.1-mini # Generate pagination URLs
# ARTICLE_PAGES_GENERATOR_MODEL=gpt-5.1-mini # Group and sample article URLs
# BATCH_EXTRACT_LISTINGS_MODEL=gpt-5.1-mini  # Batch listing extraction
# BATCH_EXTRACT_ARTICLES_MODEL=gpt-5.1-mini  # Batch article extraction
# EXTRACTION_AGENT_MODEL=gpt-5.1             # Isolated extraction context

# =============================================================================
# Example Configurations
# =============================================================================

# Cost-optimized: Use cheaper models for simple tasks
# DEFAULT_MODEL=gpt-5.1
# MAIN_AGENT_MODEL=gpt-5.1
# BROWSER_AGENT_MODEL=gpt-5.1-mini
# ACCESSIBILITY_AGENT_MODEL=gpt-5.1-mini
# LISTING_PAGES_GENERATOR_MODEL=gpt-5.1-mini
# ARTICLE_PAGES_GENERATOR_MODEL=gpt-5.1-mini
# BATCH_EXTRACT_LISTINGS_MODEL=gpt-5.1-mini
# BATCH_EXTRACT_ARTICLES_MODEL=gpt-5.1-mini

# Provider diversity: Mix OpenAI and Anthropic
# DEFAULT_MODEL=gpt-5.1
# SELECTOR_AGENT_MODEL=claude-3-5-sonnet-20241022
# LISTING_PAGE_EXTRACTOR_MODEL=claude-3-5-sonnet-20241022

# Chrome DevTools Configuration
# Option 1: Full WebSocket URL (preferred for Docker/remote)
# CDP_URL=ws://localhost:9222
# Option 2: Host/Port separately
CDP_HOST=localhost
CDP_PORT=9222
CDP_TIMEOUT=30

# Output Configuration
PLANS_OUTPUT_DIR=./plans_output
PLANS_TEMPLATE_DIR=./templates

# =============================================================================
# Storage Backend Configuration
# =============================================================================
# Backend type: "memory" (default) or "sqlalchemy" (persistent)
STORAGE_BACKEND=memory

# Database URL (only needed if STORAGE_BACKEND=sqlalchemy)
# Supports MySQL, PostgreSQL, and SQLite
# DATABASE_URL=mysql+pymysql://user:password@localhost:3306/crawler
# DATABASE_URL=postgresql://user:password@localhost:5432/crawler
# DATABASE_URL=sqlite:///./crawler.db

# To use MySQL with Docker:
# 1. Run: docker-compose up -d mysql
# 2. Run migrations: alembic upgrade head
# 3. Set: STORAGE_BACKEND=sqlalchemy
# 4. Set: DATABASE_URL=mysql+pymysql://crawler:crawler_password@localhost:3306/crawler

# =============================================================================
# Agent Version
# =============================================================================
# Version identifier for tracking (stored in session records)
AGENT_VERSION=1.0.0

# Logging Configuration
LOG_LEVEL=INFO
SERVICE_NAME=crawler-agent

LOG_CONSOLE=true
LOG_COLOR=true

OTEL_ENDPOINT=localhost:4317
OTEL_INSECURE=true